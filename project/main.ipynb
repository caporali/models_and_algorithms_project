{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### to-do list:\n",
    "- plot in task $2$ (graph/inferred graph, heatmap);\n",
    "- task $4$;\n",
    "- task $5$;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Estimation of $\\boldsymbol{J}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum likelihood estimator\n",
    "Our goal is to find the set of parameters $J_{i, j}(a, b)$ for all $i, j \\in \\{1, \\dots, N\\}$, $a, b \\in \\{1, \\dots, q\\}$ that maximises the likelihood\n",
    "\\begin{align*}\n",
    "    \\mathcal{L}\\left(\\boldsymbol{J} | \\left\\{\\boldsymbol{x}^{(m)}\\right\\}_{m = 1}^M\\right) & \\coloneqq P\\left(\\left\\{\\boldsymbol{x}^{(m)}\\right\\}_{m = 1}^M | \\boldsymbol{J}\\right)\n",
    "    = \\prod_{m = 1}^{M} P\\left(\\boldsymbol{x}^{(m)} | \\boldsymbol{J}\\right) = \\\\\n",
    "    & = \\prod_{m = 1}^{M} \\frac{1}{Z(\\boldsymbol{J})} \\exp\\left(\\sum_{i, j = 1}^N\\sum_{a, b = 1}^q J_{i, j}(a, b) \\ \\delta_{x_i^{(m)}, a} \\ \\delta_{x_j^{(m)}, b}\\right) = \\\\\n",
    "    & = \\frac{1}{Z(\\boldsymbol{J})^M} \\exp\\left(\\sum_{m = 1}^M \\sum_{i, j = 1}^N \\sum_{a, b = 1}^q J_{i, j}(a, b) \\ \\delta_{x_i^{(m)}, a}\\ \\delta_{x_j^{(m)}, b}\\right),\n",
    "\\end{align*}\n",
    "<!-- remark: we keep using the letter P because it highlight the fact that we are talking about a density, not about the measure of a set -->\n",
    "where\n",
    "\\begin{equation*}\n",
    "    Z(\\boldsymbol{J}) = \\sum_{x_1, \\dots, x_N = 1}^q \\exp \\left(\\sum_{i, j = 1}^N \\sum_{a, b = 1}^q J_{i, j}(a, b) \\ \\delta_{x_i, a} \\ \\delta_{x_j, b}\\right)\n",
    "\\end{equation*}\n",
    "is the normalization constant.\n",
    "To this aim we compute the log-likelihood (and divide by $M$), getting\n",
    "\\begin{equation*}\n",
    "    l\\left(\\boldsymbol{J} | \\left\\{\\boldsymbol{x}^{(m)}\\right\\}_{m = 1}^M\\right) \\coloneqq \\left(\\frac{1}{M} \\sum_{m=1}^M\\sum_{i, j = 1}^N\\sum_{a, b = 1}^q J_{i, j}(a, b) \\ \\delta_{x_i^{(m)}, a} \\ \\delta_{x_j^{(m)}, b}\\right) - \\log{Z(\\boldsymbol{J})}.\n",
    "\\end{equation*}\n",
    "Deriving w.r.t. $J_{i, j}(a, b)$, for fixed $i, j, a, b$, we get\n",
    "\\begin{equation*}\n",
    "\t\\frac{\\partial l\\left(\\boldsymbol{J} | \\left\\{\\boldsymbol{x}^{(m)}\\right\\}_{m = 1}^M\\right)}{\\partial J_{i,j}(a,b)} \n",
    "\t= \\left(\\frac{1}{M} \\sum_{m = 1}^M \\delta_{x_i^{(m)}, a} \\ \\delta_{x_j^{(m)}, b}\\right) - \\frac{1}{Z(\\boldsymbol{J})} \\frac{\\partial Z(\\boldsymbol{J})}{\\partial J_{i, j}(a, b)},\n",
    "\\end{equation*}\n",
    "where\n",
    "\\begin{equation*}\n",
    "\t\\frac{\\partial Z(\\boldsymbol{J})}{\\partial J_{i, j}(a, b)} = \\sum_{x_1, \\dots, x_N = 1}^q \\delta_{x_i,a} \\ \\delta_{x_j,b} \\ \\exp\\left(\\sum_{i, j = 1}^N\\sum_{a, b = 1}^q J_{i, j}(a, b) \\ \\delta_{x_i, a} \\ \\delta_{x_j, b}\\right).\n",
    "\\end{equation*}\n",
    "Plugging into the previous expression, we find that\n",
    "\\begin{align*}\n",
    "   \\frac{\\partial l\\left(\\boldsymbol{J} | \\left\\{\\boldsymbol{x}^{(m)}\\right\\}_{m = 1}^M\\right)}{\\partial J_{i,j}(a,b)} & = \\left(\\frac{1}{M} \\sum_{m = 1}^M \\delta_{x_i^{(m)}, a} \\ \\delta_{x_j^{(m)}, b}\\right) - \\sum_{x_1, \\dots, x_N = 1}^q \\delta_{x_i,a} \\ \\delta_{x_j,b} \\ \\frac{1}{Z(\\boldsymbol{J})} \\exp\\left(\\sum_{i, j = 1}^N\\sum_{a, b = 1}^q J_{i, j}(a, b) \\ \\delta_{x_i, a} \\ \\delta_{x_j, b}\\right) = \\\\\n",
    "   & = \\langle \\delta_{(x_i, x_j), (a, b)} \\rangle_{\\rm{data}} - \\langle \\delta_{(x_i, x_j), (a, b)} \\rangle_{\\rm{model}},\n",
    "\\end{align*}\n",
    "where $\\langle \\cdot \\rangle_{\\rm{data}}$ stands for the empirical mean of the observations, $\\langle \\cdot \\rangle_{\\rm{model}}$ is the mean of $\\delta_{(x_i, x_j), (a, b)}$ computed on the distribution of $\\boldsymbol{x} | \\boldsymbol{J}$ and $\\delta$ denotes again the Kronecker delta\n",
    "\\begin{equation*}\n",
    "\t\\delta_{(x_i, x_j), (a, b)} \\coloneqq \n",
    "\t\\begin{cases}\n",
    "\t\t1 & \\text{if } x_i = a \\text{ and } x_j = b \\\\\n",
    "\t\t0 & \\text{otherwise}\n",
    "\t\\end{cases}.\n",
    "\\end{equation*}\n",
    "Hence, in order to find the value of $J$ for which the function $\\mathcal{L}\\left(\\boldsymbol{J} | \\left\\{\\boldsymbol{x}^{(m)}\\right\\}_{m = 1}^M\\right)$ is maximised we have to impose\n",
    "\\begin{equation*}\n",
    "\t\\frac{\\partial l\\left(\\boldsymbol{J} | \\left\\{\\boldsymbol{x}^{(m)}\\right\\}_{m = 1}^M\\right)}{\\partial J_{i,j}(a,b)} = 0 \\iff \\langle \\delta_{(x_i, x_j), (a, b)} \\rangle_{\\rm{data}} = \\langle \\delta_{(x_i, x_j), (a, b)} \\rangle_{\\rm{model}}.\n",
    "\\end{equation*}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boltzmann machine learning scheme\n",
    "\n",
    "What we found can be exploited iteratively to estimate the coupling matrices through a gradient ascent algorithm (Boltzmann machine learning): $\\forall i, j, a, b$\n",
    "\\begin{align*}\n",
    "\t& J_{i, j}^{0}(a, b) = 0, \\\\\n",
    "\t& J_{i, j}^{t + 1}(a, b) \\leftarrow J_{i, j}^{t}(a, b) + \\lambda \\left[\\langle \\delta_{(x_i, x_j), (a, b)} \\rangle_{\\rm{data}} - \\langle \\delta_{(x_i, x_j), (a, b)} \\rangle_{\\rm{model}(t)}\\right], \\ \\forall t \\geq 0.\n",
    "\\end{align*}\n",
    "It is clear that at every step $t$ we should perform the computation of $\\langle \\delta_{(x_i, x_j), (a, b)} \\rangle_{\\rm{model(t)}}$ which costs $O(q^N)$, so we bypass the problem using a Metropolis-Hastings algorithm to sample from \n",
    "\\begin{equation*}\n",
    "\t\\pi_t\\left(\\boldsymbol{x}\\right) \\coloneqq \\frac{1}{Z(\\boldsymbol{J}^t)} \\exp\\left(\\sum_{i, j = 1}^N\\sum_{a, b = 1}^q J_{i, j}^{t}(x_i, x_j)\\right)\n",
    "\\end{equation*}\n",
    "and later estimate $\\langle \\delta_{(x_i, x_j), (a, b)} \\rangle_{\\rm{model(t)}}$:\n",
    "- `set` an initial condition $\\boldsymbol{x}^{0}$ (extract randomly from the $q^N$ possible configurations);\n",
    "- `for` $s \\in \\{1, \\dots, T_{\\rm{burn-in}} + T_{\\rm{tot}} \\times T_{\\rm{wait}}\\}$:\n",
    "\t1. `draw` $\\boldsymbol{x} \\sim p\\left(\\boldsymbol{x}|\\boldsymbol{x}^{(s - 1)}\\right)$ with \n",
    "\t\\begin{align*}\n",
    "\t\tp\\left(\\boldsymbol{x}|\\boldsymbol{x}^{(s - 1)}\\right) = \n",
    "\t\t\\begin{cases}\n",
    "\t\t\t\\frac{1}{qN} & \\text{if } \\boldsymbol{x} = \\left(x^{(s - 1)}_1, \\dots, x^{(s - 1)}_{i - 1}, \\left(x^{(s - 1)}_{i} + p\\right) \\text{ mod } q, x^{(s - 1)}_{i + 1}, \\dots, x^{(s - 1)}_{N}\\right), \\ \\forall i \\in {1, \\dots, N}, \\forall p \\in {1, \\dots, q}\\\\\n",
    "\t\t\t0 & \\text{otherwise}\n",
    "\t\t\\end{cases};\n",
    "\t\\end{align*}\n",
    "\t2. `compute` the acceptance ratio $a\\left(\\boldsymbol{x}|\\boldsymbol{x}^{(s - 1)}\\right)$:\n",
    "\t\\begin{align*}\n",
    "\t\ta\\left(\\boldsymbol{x}|\\boldsymbol{x}^{(s - 1)}\\right) & = \\min\\left[1, \\frac{p\\left(\\boldsymbol{x}^{(s - 1)}|\\boldsymbol{x}\\right) \\pi_t(x)}{p\\left(\\boldsymbol{x}|\\boldsymbol{x}^{(s - 1)}\\right) \\pi_t(x^{(s - 1)})}\\right] = \\\\\n",
    "\t\t& = \\min\\left[1, \\mathbf{1}_A(\\boldsymbol{x}) \\exp\\left(\\sum_{i, j = 1}^N J_{i, j}^{t}\\left(x_i, x_j\\right) - J_{i, j}^{t}\\left(x^{(s - 1)}_i, x^{(s - 1)}_j\\right)\\right)\\right],\n",
    "\t\\end{align*}\n",
    "\twhere we adopt the convention $\\frac{p\\left(\\boldsymbol{x}^{(s - 1)}|\\boldsymbol{x}\\right)}{p\\left(\\boldsymbol{x}|\\boldsymbol{x}^{(s - 1)}\\right)} = \\mathbf{1}_A(\\boldsymbol{x})$ with $A \\coloneqq \\left\\{x \\,|\\, p\\left(\\boldsymbol{x}|\\boldsymbol{x}^{(s - 1)}\\right) > 0\\right\\}$ (this notation has only a theoretical purpose).\n",
    "\tNow assuming \n",
    "\t\\begin{equation*}\n",
    "\t\t\\boldsymbol{x} = \\left(x^{(s - 1)}_1, \\dots, x^{(s - 1)}_{k - 1}, \\left(x^{(s - 1)}_{k} + p\\right) \\text{ mod } q, x^{(s - 1)}_{k + 1}, \\dots, x^{(s - 1)}_{N}\\right),\n",
    "\t\\end{equation*}\n",
    "\twe have\n",
    "\t\\begin{align*}\n",
    "\t\ta\\left(\\boldsymbol{x}|\\boldsymbol{x}^{(s - 1)}\\right) = \\min\\Bigg[1, & \\exp\\Bigg(\\sum_{i \\neq k} J_{i, k}^{t}\\left(x^{(s - 1)}_i, \\left(x^{(s - 1)}_{k} \\pm 1\\right) \\text{ mod } q\\right) - J_{i, k}^{t}\\left(x^{(s - 1)}_i, x^{(s - 1)}_k\\right) + \\\\\n",
    "\t\t& + \\sum_{j \\neq k} J_{k, j}^{t}\\left(\\left(x^{(s - 1)}_{k} + p\\right) \\text{ mod } q, x^{(s - 1)}_j\\right) - J_{k, j}^{t}\\left(x^{(s - 1)}_k, x^{(s - 1)}_j\\right) + \\\\\n",
    "\t\t& + \\left(J_{k, k}^{t}\\left(\\left(x^{(s - 1)}_{k} + p\\right) \\text{ mod } q, \\left(x^{(s - 1)}_{k} + p\\right) \\text{ mod } q\\right) - J_{k, k}^{t}\\left(x^{(s - 1)}_k, x^{(s - 1)}_k\\right)\\right) \\Bigg)\\Bigg].\n",
    "\t\\end{align*}\n",
    "\tBy simmetry of $\\boldsymbol{J}$, i.e. $J_{i, j}(a, b) = J_{j, i}(b, a)$, it holds\n",
    "\t\\begin{align*}\n",
    "\t\ta\\left(\\boldsymbol{x}|\\boldsymbol{x}^{(s - 1)}\\right) = \\min\\Bigg[1, & \\exp\\Bigg(2 \\sum_{i \\neq k} J_{i, k}^{t}\\left(x^{(s - 1)}_i, \\left(x^{(s - 1)}_{k} + p\\right) \\text{ mod } q\\right) - J_{i, k}^{t}\\left(x^{(s - 1)}_i, x^{(s - 1)}_k\\right) + \\\\\n",
    "\t\t& + \\left(J_{k, k}^{t}\\left(\\left(x^{(s - 1)}_{k} + p\\right) \\text{ mod } q, \\left(x^{(s - 1)}_{k} + p\\right) \\text{ mod } q\\right) - J_{k, k}^{t}\\left(x^{(s - 1)}_k, x^{(s - 1)}_k\\right)\\right) \\Bigg)\\Bigg].\n",
    "\t\\end{align*}\n",
    "\t3. `draw` $u \\sim U[0,1)$ (with the command `rand()`);\n",
    "\t4. `set`\n",
    "\t\\begin{equation*}\n",
    "\t\t\\boldsymbol{x}^{(s)} \\coloneqq \n",
    "\t\t\\begin{cases}\n",
    "\t\t\t\\boldsymbol{x} & \\text{if } u \\leq a \\\\\n",
    "\t\t\t\\boldsymbol{x}^{(s - 1)} & \\text{otherwise}\n",
    "\t\t\\end{cases};\n",
    "\t\\end{equation*}\n",
    "- estimate $\\langle \\delta_{(x_i, x_j), (a, b)} \\rangle_{\\rm{model(t)}}$ with $T_{\\rm{tot}}$ configurations obtained removing the burn-in and the waiting times:\n",
    "\\begin{equation*}\n",
    "\t\\langle \\delta_{(x_i, x_j), (a, b)} \\rangle_{\\rm{model(t)}} \\sim \\frac{1}{T_{\\rm{tot}}} \\sum_{s = 1}^{T_{\\rm{tot}}} \\delta_{(x^{(s)}_i, x^{(s)}_j), (a, b)}.\n",
    "\\end{equation*}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages\n",
    "using ProgressMeter\n",
    "using Distributions\n",
    "using DelimitedFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "function compute_stat(data::Matrix{Int64}, q::Int)\n",
    "\tcount = size(data, 1)\n",
    "\tn = size(data, 2)\n",
    "\tdelta_ijab = (zeros(q, q), ) .* ones(n, n)\n",
    "\n",
    "\tfor i in 1:n, j in 1:n, a in 1:q, b in 1:q\n",
    "\t\tfor s in 1:count\n",
    "\t\t\tdelta_ijab[i, j][a, b] += (data[s, i] == a) * (data[s, j] == b)\n",
    "\t\tend\n",
    "\tend\n",
    "\tdelta_ijab /= count\n",
    "\n",
    "\treturn delta_ijab\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "function maxabs_matmat(m::Matrix{Matrix{Float64}})\n",
    "\tmax = -1\n",
    "\tfor r in 1:size(m, 1), c in 1:size(m, 2)\n",
    "\t\tif maximum(abs.(m[r, c])) > max\n",
    "\t\t\tmax = maximum(abs.(m[r, c]))\n",
    "\t\tend\n",
    "\tend\n",
    "\n",
    "\treturn max\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "function metropolis_hastings_step(x::Vector{Int64}, J::Matrix{Matrix{Float64}})\n",
    "\tn = length(x)\n",
    "\n",
    "\t# 1.\n",
    "\tk = rand(1:n)\n",
    "\tp = rand(1:q)\n",
    "\t\n",
    "\t# 2.\n",
    "\ta = 0\n",
    "\txk_new = mod((x[k] + p) - 1, q) + 1\n",
    "\tfor i in 1:n\n",
    "\t\tif i != k\n",
    "\t\t\ta += J[i, k][x[i], xk_new] - J[i, k][x[i], x[k]]\n",
    "\t\tend\n",
    "\tend\n",
    "\ta *= 2\n",
    "\ta += J[k, k][xk_new, xk_new] - J[k, k][x[k], x[k]]\n",
    "\ta = exp(a)\n",
    "\ta = min(1, a)\n",
    "\n",
    "\t# 3/4.\n",
    "\tif rand() < a\n",
    "\t\t# mod with indexes starting from 1\n",
    "\t\tx[k] = xk_new\n",
    "\tend\n",
    "\t\n",
    "\treturn x\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "function boltzmann_ml(delta_ijab_data::Matrix{Matrix{Float64}}, J::Matrix{Matrix{Float64}}, \n",
    "\t\t\t\t\tt_burnin::Int64, t_tot::Int64, t_wait::Int64, t_max::Int64;\n",
    "\t\t\t\t\tλ::Float64 = 0.1, ε_max::Float64 = 1e-2)\n",
    "\tn = size(delta_ijab_data, 1)\n",
    "\tq = size(delta_ijab_data[1, 1], 1)\n",
    "\t\n",
    "\tx = sample(collect(1:q), n, replace = true)\n",
    "\tx_model = zeros(Int64, t_tot, n)\n",
    "\tdelta_ijab_model = (zeros(q, q), ) .* ones(n, n)\n",
    "\n",
    "\tt = 0\n",
    "\tε = 1\n",
    "\tProgressMeter.ijulia_behavior(:clear)\n",
    "\tp = ProgressUnknown(\"learning...\", spinner = true)\n",
    "\n",
    "\twhile t <= t_max && ε > ε_max\n",
    "\t\tt += 1\n",
    "\t\tfill!(x_model, 0)\n",
    "\t\tx = sample(collect(1:q), n, replace = true)\n",
    "\n",
    "\t\tfor s in 1:t_burnin\n",
    "\t\t\tx = metropolis_hastings_step(x, J)\n",
    "\t\tend\n",
    "\t\tfor s in 1:t_tot\n",
    "\t\t\tfor r in 1:t_wait\n",
    "\t\t\t\tx = metropolis_hastings_step(x, J)\n",
    "\t\t\tend\n",
    "\t\t\tx_model[s, :] = x\n",
    "\t\tend\n",
    "\t\t\n",
    "\t\tdelta_ijab_model = compute_stat(x_model, q)\n",
    "\t\tJ = J + λ .* (delta_ijab_data - delta_ijab_model)\n",
    "\t\n",
    "\t\tε = maxabs_matmat(delta_ijab_data - delta_ijab_model)\n",
    "\n",
    "\t\tif mod(t, t_max ÷ 10) == 0\n",
    "\t\t\tProgressMeter.next!(p; showvalues = [(:t, t), (:ε, ε)], spinner = \"⠋⠙⠹⠸⠼⠴⠦⠧⠇⠏\")\n",
    "\t\tend\n",
    "\tend\n",
    "\n",
    "\tProgressMeter.finish!(p)\n",
    "\n",
    "\treturn delta_ijab_model, J, x_model, t, ε\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "n = 5\n",
    "q = 4\n",
    "\n",
    "# the estimation is also acceptable by imposing\n",
    "# t_burnin = 250\n",
    "# t_tot = 250\n",
    "# t_wait = 100\n",
    "# t_max = 500\n",
    "\n",
    "t_burnin = 500\n",
    "t_tot = 500\n",
    "t_wait = 100\n",
    "t_max = 500;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delta_ijab_data\n",
    "x_data = readdlm(\"data.dat\", Int)\n",
    "delta_ijab_data = compute_stat(x_data, q);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[32m⠋ learning... \t Time: 0:00:06\u001b[39m\u001b[K\r\n",
      "\u001b[34m  t:  100\u001b[39m\u001b[K\r\n",
      "\u001b[34m  ε:  0.040999999999999995\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[K\u001b[A\r\u001b[K\u001b[A\r\u001b[32m⠙ learning... \t Time: 0:00:10\u001b[39m\u001b[K\r\n",
      "\u001b[34m  t:  150\u001b[39m\u001b[K\r\n",
      "\u001b[34m  ε:  0.028999999999999998\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[K\u001b[A\r\u001b[K\u001b[A\r\u001b[32m⠹ learning... \t Time: 0:00:13\u001b[39m\u001b[K\r\n",
      "\u001b[34m  t:  200\u001b[39m\u001b[K\r\n",
      "\u001b[34m  ε:  0.04099999999999998\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[K\u001b[A\r\u001b[K\u001b[A\r\u001b[32m⠸ learning... \t Time: 0:00:15\u001b[39m\u001b[K\r\n",
      "\u001b[34m  t:  250\u001b[39m\u001b[K\r\n",
      "\u001b[34m  ε:  0.024499999999999994\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[K\u001b[A\r\u001b[K\u001b[A\r\u001b[32m⠼ learning... \t Time: 0:00:18\u001b[39m\u001b[K\r\n",
      "\u001b[34m  t:  300\u001b[39m\u001b[K\r\n",
      "\u001b[34m  ε:  0.04200000000000001\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[K\u001b[A\r\u001b[K\u001b[A\r\u001b[32m⠴ learning... \t Time: 0:00:21\u001b[39m\u001b[K\r\n",
      "\u001b[34m  t:  350\u001b[39m\u001b[K\r\n",
      "\u001b[34m  ε:  0.037500000000000006\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[K\u001b[A\r\u001b[K\u001b[A\r\u001b[32m⠦ learning... \t Time: 0:00:23\u001b[39m\u001b[K\r\n",
      "\u001b[34m  t:  400\u001b[39m\u001b[K\r\n",
      "\u001b[34m  ε:  0.03249999999999997\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[K\u001b[A\r\u001b[K\u001b[A\r\u001b[32m⠧ learning... \t Time: 0:00:26\u001b[39m\u001b[K\r\n",
      "\u001b[34m  t:  450\u001b[39m\u001b[K\r\n",
      "\u001b[34m  ε:  0.05149999999999999\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[K\u001b[A\r\u001b[K\u001b[A\r\u001b[32m⠇ learning... \t Time: 0:00:28\u001b[39m\u001b[K\r\n",
      "\u001b[34m  t:  500\u001b[39m\u001b[K\r\n",
      "\u001b[34m  ε:  0.058499999999999996\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[K\u001b[A\r\u001b[K\u001b[A\r\u001b[32m✓ learning... \t Time: 0:00:28\u001b[39m\u001b[K\n"
     ]
    }
   ],
   "source": [
    "# boltzmann ml scheme\n",
    "J = (zeros(q, q), ) .* ones(n, n)\n",
    "delta_ijab_model, J, x_model, t, ε = boltzmann_ml(delta_ijab_data, J, t_burnin, t_tot, t_wait, t_max);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Computation of $\\mathcal{F}_{i, j}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frobenius norm\n",
    "\n",
    "In the previous point we estimated the parameter $\\boldsymbol{J}$ through a Boltzmann machine learning scheme and now we call that parameter $\\boldsymbol{J}^\\star$.\n",
    "\n",
    "We define for $i, j \\in \\{1, \\dots, N\\}$ the Frobenius norm of $J_{i, j}^{\\star}$ as\n",
    "\\begin{equation*}\n",
    "\t\\mathcal{F}_{i, j} = \\sqrt{\\sum_{a, b = 1}^{q} J^{\\star}_{i, j}(a, b)^2}.\n",
    "\\end{equation*}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages\n",
    "using Printf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "function frobenius_norm(m::Matrix{Float64})\n",
    "\tf = 0\n",
    "\tfor r in 1:size(m, 1), c in 1:size(m, 2)\n",
    "\t\tf += m[r, c]^2\n",
    "\tend\n",
    "\tf = sqrt(f)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×5 Matrix{Float64}:\n",
       " 0.0165351  0.743991   0.836902   0.169108   0.112357\n",
       " 0.743991   0.0334248  0.123597   0.819493   0.108487\n",
       " 0.836902   0.123597   0.0139759  0.1028     0.877946\n",
       " 0.169108   0.819493   0.1028     0.0404455  0.915088\n",
       " 0.112357   0.108487   0.877946   0.915088   0.021862"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "5×2 Matrix{Int64}:\n",
       " 2  3\n",
       " 1  4\n",
       " 1  5\n",
       " 2  5\n",
       " 3  4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# computation of f_{i, j} for all i, j\n",
    "f = zeros(n, n)\n",
    "for i in 1:n, j in 1:n\n",
    "\tf[i, j] = frobenius_norm(J[i, j])\n",
    "end\n",
    "display(f)\n",
    "\n",
    "neighbors = [Int64[], Int64[], Int64[], Int64[], Int64[]]\n",
    "for i in 1:n, j in 1:n\n",
    "\tif (f[i, j] > 0.2)\n",
    "\t\tappend!(neighbors[i], [j])\n",
    "\tend\n",
    "end\n",
    "neighbors = mapreduce(permutedims, vcat, neighbors)\n",
    "display(neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×2 Matrix{Int64}:\n",
       " 2  3\n",
       " 1  4\n",
       " 1  5\n",
       " 2  5\n",
       " 3  4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "groundtruth = readdlm(\"groundtruth.dat\", Int)\n",
    "display(groundtruth)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bayesian approach"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posterior density\n",
    "\n",
    "From the bayesian theory we recall that the posterior distribution of the parameter $\\boldsymbol{J}$ (up to a positive multiplicative constant) is such that\n",
    "\\begin{align*}\n",
    "\tP\\left(\\boldsymbol{J} | \\left\\{x^{(m)}\\right\\}_{m = 1}^M \\right) & \\propto P\\left(\\left\\{x^{(m)}\\right\\}_{m = 1}^M | \\boldsymbol{J}\\right) P\\left(\\boldsymbol{J}\\right) = \\\\\n",
    "\t& = \\frac{1}{Z(\\boldsymbol{J})^M} \\exp\\left(\\sum_{m = 1}^M \\sum_{i, j = 1}^N \\sum_{a, b = 1}^q J_{i, j}(a, b) \\ \\delta_{x_i^{(m)}, a}\\ \\delta_{x_j^{(m)}, b}\\right)\\frac{1}{Z\\left(\\boldsymbol{J}\\right)}\\exp\\left(-\\lambda \\sum_{i, j = 1}^N \\sum_{a, b = 1}^{q} \\left|J_{i, j}(a, b)\\right|\\right) = \\\\\n",
    "\t& = \\frac{1}{Z\\left(\\boldsymbol{J}\\right)^{M + 1}} \\exp\\left(\\sum_{i, j = 1}^N \\left(\\sum_{m = 1}^M J_{i, j}(x_i^{(m)}, x_j^{(m)}) - \\lambda \\sum_{a, b = 1}^{q} \\left|J_{i, j}(a, b)\\right|\\right)\\right),\n",
    "\\end{align*}\n",
    "where the previous chain of equations follows from the definition of conditional density.\n",
    "\n",
    "Our objective is to modify the Boltzmann machine learning scheme in order to include the posterior distribution just computed.\n",
    "This can be done in several ways by substituting the classical maximum likelihood estimation of the parameter $\\boldsymbol{J}$ with a Bayes estimation. To do so we have to set up a decision theory framework. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum a posteriori estimation\n",
    "\n",
    "We follow the classical Bayesian approach and introduce a loss function $L(\\theta, \\hat{\\theta})$ and its related risk function $R(\\hat{\\theta})$ as follows\n",
    "\\begin{align*}\n",
    "\t& L(\\theta, \\hat{\\theta}) \\coloneqq 1 - \\delta(\\hat{\\theta} - \\theta) \\\\\n",
    "\t& \\begin{aligned}\n",
    "\t\tR(\\hat{\\theta}) & \\coloneqq \\mathbb{E}_{\\theta | x}\\left[L(\\theta, \\hat{\\theta})\\right] = \\int_{\\Theta} L(\\theta, \\hat{\\theta}) p(\\theta | x) d\\theta = \\\\\n",
    "\t\t& \\ = 1 - \\int_{\\Theta} \\delta(\\hat{\\theta} - \\theta) p(\\theta | x) d\\theta = \\\\\n",
    "\t\t& \\ = 1 - p(\\theta | x),\n",
    "\t\\end{aligned}\n",
    "\\end{align*}\n",
    "where $\\delta$ is the dirac delta function (here we are committing a huge abuse of notation: we are using $\\delta$ as the Radon-Nikodym derivative of the dirac measure but it does not exists such a derivative; we will formalize it in the very end of this section). We used the so-called $0-1$-loss function in order to obtain an eastimator easy to compute. \n",
    "\n",
    "Hence, by minimizing the risk function, one obtains a maximum a posteriori (MAP) estimate of $\\theta$ which is\n",
    "\\begin{align*}\n",
    "\t\\hat{\\theta}_{\\rm{MAP}} & = \\operatorname*{argmin}_{\\theta \\in \\Theta} R(\\theta) = \\\\\n",
    "\t& = \\operatorname*{argmin}_{\\theta \\in \\Theta} 1 - p(\\theta | x) = \\\\\n",
    "\t& = \\operatorname*{argmax}_{\\theta \\in \\Theta} p(\\theta | x) = \\\\\n",
    "\t& = \\operatorname*{argmax}_{\\theta \\in \\Theta} \\frac{p(x | \\theta) p(\\theta)}{\\int_{\\Theta} p(x | \\theta) p(\\theta) d\\theta} = \\\\\n",
    "\t& = \\operatorname*{argmax}_{\\theta \\in \\Theta} p(x | \\theta) p(\\theta).\n",
    "\\end{align*}\n",
    "\n",
    "So defining a new *likelihood function* $\\mathcal{L}_{\\rm{MAP}}(\\theta) \\coloneqq p(x | \\theta) p(\\theta)$ one obtains\n",
    "\\begin{equation*}\n",
    "\t\\hat{\\theta}_{\\rm{MAP}} = \\operatorname*{argmax}_{\\theta \\in \\Theta} \\mathcal{L}_{\\rm{MAP}}(\\theta).\n",
    "\\end{equation*}\n",
    "Moreover, if $l_{\\rm{MAP}}(\\theta) \\coloneqq \\log{\\mathcal{L}_{\\rm{MAP}}(\\theta)}$, it holds\n",
    "\\begin{equation*}\n",
    "\tl_{\\rm{MAP}}(\\theta) = \\log{p(x | \\theta) p(\\theta)} = l_{\\rm{ML}}(\\theta) + \\log{p(\\theta)},\n",
    "\\end{equation*}\n",
    "where $l_{\\rm{ML}}(\\theta)$ is the classical log-likelihood function used in maximum likelihood estimation.\n",
    "\n",
    "Actually, in order to be formal, one should define a family of loss function $\\forall \\varepsilon > 0$ as follows:\n",
    "\\begin{align*}\n",
    "\t& L_{\\varepsilon}(\\theta, \\hat{\\theta}) \\coloneqq 1 - \\mathbf{1}_{(-\\varepsilon, \\varepsilon)}(\\hat{\\theta} - \\theta) \\\\\n",
    "\t& \\begin{aligned}\n",
    "\t\tR_{\\varepsilon}(\\hat{\\theta}) & = \\mathbb{E}_{\\theta | x}\\left[L_{\\varepsilon}(\\theta, \\hat{\\theta})\\right] = \\\\\n",
    "\t\t& = 1 - \\int_{-\\hat{\\theta} - \\varepsilon}^{\\hat{\\theta} + \\varepsilon} p(\\theta | x) d\\theta.\n",
    "\t\\end{aligned}\n",
    "\\end{align*}\n",
    "Then, by taking $\\varepsilon \\to 0$, one obtains the risk used above, i.e. $R(\\hat{\\theta}) = 1 - p(\\hat{\\theta} | x)$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimation of $\\boldsymbol{J}$ with MAP\n",
    "\n",
    "Following the procedure just introduced we can compute $l_{\\rm{MAP}}(\\boldsymbol{J})$\n",
    "\\begin{align*}\n",
    "\tl_{\\rm{MAP}}(\\boldsymbol{J}) & = \\log\\left(P\\left(\\left\\{x^{(m)}\\right\\}_{m = 1}^M | \\boldsymbol{J}\\right) P\\left(\\boldsymbol{J}\\right)\\right)= \\\\\n",
    "\t& = \\log\\left(\\frac{1}{Z\\left(\\boldsymbol{J}\\right)^{M + 1}} \\exp\\left(\\sum_{i, j = 1}^N \\sum_{a, b = 1}^q \\left(\\sum_{m = 1}^M J_{i, j}(a, b) \\ \\delta_{x_i^{(m)}, a}\\ \\delta_{x_j^{(m)}, b} - \\lambda \\left|J_{i, j}(a, b)\\right|\\right)\\right)\\right) = \\\\\n",
    "\t& = \\sum_{i, j = 1}^N \\sum_{a, b = 1}^q \\left(\\sum_{m = 1}^M J_{i, j}(a, b) \\ \\delta_{x_i^{(m)}, a}\\ \\delta_{x_j^{(m)}, b} - \\lambda \\left|J_{i, j}(a, b)\\right|\\right) - (M + 1) \\log{Z\\left(\\boldsymbol{J}\\right)}.\n",
    "\\end{align*}\n",
    "Then we derive it w.r.t. $J_{i, j}(a, b)$, for fixed $i, j, a, b$, assuming that $\\frac{d |x|}{dx} = \\operatorname{sgn}(x)$ where, by convention, we assume $\\operatorname{sgn}(0) = 1$:\n",
    "\\begin{equation*}\n",
    "    \\frac{\\partial l_{\\rm{MAP}}(\\boldsymbol{J})}{\\partial J_{i,j}(a,b)} \n",
    "\t= \\left(\\sum_{m = 1}^M \\delta_{x_i^{(m)}, a} \\ \\delta_{x_j^{(m)}, b} - \\lambda \\operatorname{sgn}\\left(J_{i, j}(a, b)\\right)\\right) - \\frac{M + 1}{Z(\\boldsymbol{J})} \\frac{\\partial Z(\\boldsymbol{J})}{\\partial J_{i, j}(a, b)},\n",
    "\\end{equation*}\n",
    "where we recall that\n",
    "\\begin{equation*}\n",
    "    \\frac{\\partial Z(\\boldsymbol{J})}{\\partial J_{i, j}(a, b)} = \\sum_{x_1, \\dots, x_N = 1}^q \\delta_{x_i,a} \\ \\delta_{x_j,b} \\ \\exp\\left(\\sum_{i, j = 1}^N\\sum_{a, b = 1}^q J_{i, j}(a, b) \\ \\delta_{x_i, a} \\ \\delta_{x_j, b}\\right).\n",
    "\\end{equation*}\n",
    "Dividing by $M + 1$ (observe that this operation is safe because a scaling does not modify the *direction* of the derivative) we obtain\n",
    "\\begin{align*}\n",
    "    \\frac{1}{M + 1}\\frac{\\partial l_{\\rm{MAP}}(\\boldsymbol{J})}{\\partial J_{i,j}(a,b)} & = \\left(\\frac{1}{M + 1} \\sum_{m = 1}^M \\delta_{x_i^{(m)}, a} \\ \\delta_{x_j^{(m)}, b}\\right) - \\left(\\frac{1}{M + 1} \\lambda \\operatorname{sgn}\\left(J_{i, j}(a, b)\\right)\\right) + \\\\\n",
    "\t& \\kern{13pt} - \\sum_{x_1, \\dots, x_N = 1}^q \\delta_{x_i,a} \\ \\delta_{x_j,b} \\ \\frac{1}{Z(\\boldsymbol{J})} \\exp\\left(\\sum_{i, j = 1}^N\\sum_{a, b = 1}^q J_{i, j}(a, b) \\ \\delta_{x_i, a} \\ \\delta_{x_j, b}\\right) = \\\\\n",
    "\t& = \\frac{M + 1}{M} \\langle \\delta_{(x_i, x_j), (a, b)} \\rangle_{\\rm{data}} - \\langle \\delta_{(x_i, x_j), (a, b)} \\rangle_{\\rm{model}} - \\left(\\frac{1}{M + 1} \\lambda \\operatorname{sgn}\\left(J_{i, j}(a, b)\\right)\\right).\n",
    "\\end{align*}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boltzmann machine learning scheme\n",
    "\n",
    "As we did in point 1 we set up another Boltzmann machine learning scheme in order to infer $\\boldsymbol{J}$ by taking in account the prior distribution of $P\\left(\\boldsymbol{J}\\right)$: $\\forall i, j, a, b$\n",
    "\\begin{align*}\n",
    "\t& J_{i, j}^{0}(a, b) = 0, \\\\\n",
    "\t& J_{i, j}^{t + 1}(a, b) \\leftarrow J_{i, j}^{t}(a, b) + \\mu \\left[\\frac{1}{M + 1}\\frac{\\partial l_{\\rm{MAP}}(\\boldsymbol{J})}{\\partial J_{i,j}(a,b)}\\right], \\ \\forall t \\geq 0.\n",
    "\\end{align*}\n",
    "Note that we are already able to compute all the terms of $\\frac{1}{M + 1}\\frac{\\partial l_{\\rm{MAP}}(\\boldsymbol{J})}{\\partial J_{i,j}(a,b)}$ so we can directly proceed with the numerical estimation of $\\boldsymbol{J}$. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "function sgn(x::Float64)\n",
    "\tif x >= 0\n",
    "\t\treturn 1\n",
    "\telse\n",
    "\t\treturn -1\n",
    "\tend\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "function boltzmann_ml_bayes(delta_ijab_data::Matrix{Matrix{Float64}}, J::Matrix{Matrix{Float64}},\n",
    "\t\t\t\t\tcount::Int64, λ::Int64, t_burnin::Int64, t_tot::Int64, t_wait::Int64, t_max::Int64;\n",
    "\t\t\t\t\tμ::Float64 = 0.1, ε_max::Float64 = 1e-2)\n",
    "\tn = size(delta_ijab_data, 1)\n",
    "\tq = size(delta_ijab_data[1, 1], 1)\n",
    "\n",
    "\tbayes_term = (zeros(q, q), ) .* ones(n, n)\n",
    "\tfor i in 1:n, j in 1:n, a in 1:q, b in 1:q\n",
    "\t\tbayes_term[i, j][a, b] = 1 / (count + 1) * λ * sgn(J[i, j][a, b])\n",
    "\tend\n",
    "\t\n",
    "\tx = sample(collect(1:q), n, replace = true)\n",
    "\tx_model = zeros(Int64, t_tot, n)\n",
    "\tdelta_ijab_model = (zeros(q, q), ) .* ones(n, n)\n",
    "\n",
    "\tt = 0\n",
    "\tε = 1\n",
    "\tProgressMeter.ijulia_behavior(:clear)\n",
    "\tp = ProgressUnknown(\"learning...\", spinner = true)\n",
    "\n",
    "\twhile t <= t_max && ε > ε_max\n",
    "\t\tt += 1\n",
    "\t\tfill!(x_model, 0)\n",
    "\t\tx = sample(collect(1:q), n, replace = true)\n",
    "\n",
    "\t\tfor s in 1:t_burnin\n",
    "\t\t\tx = metropolis_hastings_step(x, J)\n",
    "\t\tend\n",
    "\t\tfor s in 1:t_tot\n",
    "\t\t\tfor r in 1:t_wait\n",
    "\t\t\t\tx = metropolis_hastings_step(x, J)\n",
    "\t\t\tend\n",
    "\t\t\tx_model[s, :] = x\n",
    "\t\tend\n",
    "\t\t\n",
    "\t\tdelta_ijab_model = compute_stat(x_model, q)\n",
    "\n",
    "\t\tdirection_bayes = ((count + 1) / count * delta_ijab_data - delta_ijab_model - bayes_term)\n",
    "\t\tJ = J + μ .* direction_bayes\n",
    "\t\n",
    "\t\tε = maxabs_matmat(direction_bayes)\n",
    "\n",
    "\t\tif mod(t, t_max ÷ 10) == 0\n",
    "\t\t\tProgressMeter.next!(p; showvalues = [(:t, t), (:ε, ε)], spinner = \"⠋⠙⠹⠸⠼⠴⠦⠧⠇⠏\")\n",
    "\t\tend\n",
    "\tend\n",
    "\n",
    "\tProgressMeter.finish!(p)\n",
    "\n",
    "\treturn delta_ijab_model, J, x_model, t, ε\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[32m⠋ learning... \t Time: 0:00:05\u001b[39m\u001b[K\r\n",
      "\u001b[34m  t:  100\u001b[39m\u001b[K\r\n",
      "\u001b[34m  ε:  0.034\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[K\u001b[A\r\u001b[K\u001b[A\r\u001b[32m⠙ learning... \t Time: 0:00:08\u001b[39m\u001b[K\r\n",
      "\u001b[34m  t:  150\u001b[39m\u001b[K\r\n",
      "\u001b[34m  ε:  0.0595\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[K\u001b[A\r\u001b[K\u001b[A\r\u001b[32m⠹ learning... \t Time: 0:00:10\u001b[39m\u001b[K\r\n",
      "\u001b[34m  t:  200\u001b[39m\u001b[K\r\n",
      "\u001b[34m  ε:  0.07150000000000001\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[K\u001b[A\r\u001b[K\u001b[A\r\u001b[32m⠸ learning... \t Time: 0:00:13\u001b[39m\u001b[K\r\n",
      "\u001b[34m  t:  250\u001b[39m\u001b[K\r\n",
      "\u001b[34m  ε:  0.04149999999999998\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[K\u001b[A\r\u001b[K\u001b[A\r\u001b[32m⠼ learning... \t Time: 0:00:16\u001b[39m\u001b[K\r\n",
      "\u001b[34m  t:  300\u001b[39m\u001b[K\r\n",
      "\u001b[34m  ε:  0.03\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[K\u001b[A\r\u001b[K\u001b[A\r\u001b[32m⠴ learning... \t Time: 0:00:19\u001b[39m\u001b[K\r\n",
      "\u001b[34m  t:  350\u001b[39m\u001b[K\r\n",
      "\u001b[34m  ε:  0.04250000000000001\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[K\u001b[A\r\u001b[K\u001b[A\r\u001b[32m⠦ learning... \t Time: 0:00:21\u001b[39m\u001b[K\r\n",
      "\u001b[34m  t:  400\u001b[39m\u001b[K\r\n",
      "\u001b[34m  ε:  0.05049999999999999\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[K\u001b[A\r\u001b[K\u001b[A\r\u001b[32m⠧ learning... \t Time: 0:00:25\u001b[39m\u001b[K\r\n",
      "\u001b[34m  t:  450\u001b[39m\u001b[K\r\n",
      "\u001b[34m  ε:  0.04349999999999998\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[K\u001b[A\r\u001b[K\u001b[A\r\u001b[32m⠇ learning... \t Time: 0:00:27\u001b[39m\u001b[K\r\n",
      "\u001b[34m  t:  500\u001b[39m\u001b[K\r\n",
      "\u001b[34m  ε:  0.04949999999999999\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[K\u001b[A\r\u001b[K\u001b[A\r\u001b[32m✓ learning... \t Time: 0:00:27\u001b[39m\u001b[K\n"
     ]
    }
   ],
   "source": [
    "# boltzmann ml scheme\n",
    "count = size(x_data, 1)\n",
    "λ = 0.1\n",
    "J_bayes = (zeros(q, q), ) .* ones(n, n)\n",
    "delta_ijab_model_bayes, J_bayes, t_bayes, ε_bayes = boltzmann_ml(delta_ijab_data, J, t_burnin, t_tot, t_wait, t_max);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×5 Matrix{Float64}:\n",
       " 0.0179031  0.758188   0.878974   0.170621   0.116183\n",
       " 0.758188   0.0171435  0.133338   0.841412   0.11585\n",
       " 0.878974   0.133338   0.0301818  0.112724   0.919731\n",
       " 0.170621   0.841412   0.112724   0.0320315  0.943178\n",
       " 0.116183   0.11585    0.919731   0.943178   0.0126198"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "5×2 Matrix{Int64}:\n",
       " 2  3\n",
       " 1  4\n",
       " 1  5\n",
       " 2  5\n",
       " 3  4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# replication of point 2 with J_bayes\n",
    "f_bayes = zeros(n, n)\n",
    "for i in 1:n, j in 1:n\n",
    "\tf_bayes[i, j] = frobenius_norm(J_bayes[i, j])\n",
    "end\n",
    "display(f_bayes)\n",
    "\n",
    "neighbors_bayes = [Int64[], Int64[], Int64[], Int64[], Int64[]]\n",
    "for i in 1:n, j in 1:n\n",
    "\tif (f_bayes[i, j] > 0.2)\n",
    "\t\tappend!(neighbors_bayes[i], [j])\n",
    "\tend\n",
    "end\n",
    "neighbors_bayes = mapreduce(permutedims, vcat, neighbors_bayes)\n",
    "display(neighbors_bayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×5 Matrix{Float64}:\n",
       " 0.0179031  0.758188   0.878974   0.170621   0.116183\n",
       " 0.758188   0.0171435  0.133338   0.841412   0.11585\n",
       " 0.878974   0.133338   0.0301818  0.112724   0.919731\n",
       " 0.170621   0.841412   0.112724   0.0320315  0.943178\n",
       " 0.116183   0.11585    0.919731   0.943178   0.0126198"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "5×5 Matrix{Float64}:\n",
       " 0.0165351  0.743991   0.836902   0.169108   0.112357\n",
       " 0.743991   0.0334248  0.123597   0.819493   0.108487\n",
       " 0.836902   0.123597   0.0139759  0.1028     0.877946\n",
       " 0.169108   0.819493   0.1028     0.0404455  0.915088\n",
       " 0.112357   0.108487   0.877946   0.915088   0.021862"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2-element Vector{Float64}:\n",
       " -0.01628125437468777\n",
       "  0.04207226262184982"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.029900000000000093"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# comparison J, J_bayes\n",
    "display(f_bayes)\n",
    "display(f)\n",
    "display([minimum(f_bayes - f), maximum(f_bayes - f)])\n",
    "display(maxabs_matmat(J_bayes - J))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
