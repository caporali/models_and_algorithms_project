{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Estimation of $\\boldsymbol{J}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum likelihood estimator\n",
    "\n",
    "Our goal is to find the set of parameters $J_{i, j}(a, b)$ for all $i, j \\in \\{1, \\dots, N\\}$, $a, b \\in \\{1, \\dots, q\\}$ that maximises the likelihood\n",
    "\\begin{align*}\n",
    "    \\mathcal{L}\\left(\\boldsymbol{J}; \\left\\{\\boldsymbol{x}^{(m)}\\right\\}_{m = 1}^M\\right) & \\coloneqq \\mathbb{P}\\left(\\left\\{\\boldsymbol{x}^{(m)}\\right\\}_{m = 1}^M; \\boldsymbol{J}\\right)\n",
    "    = \\prod_{m = 1}^{M} \\mathbb{P} \\left(\\boldsymbol{x}^{(m)}; \\boldsymbol{J}\\right) = \\\\\n",
    "    & = \\prod_{m = 1}^{M} \\frac{1}{Z(\\boldsymbol{J})} \\exp\\left(\\sum_{i, j = 1}^N\\sum_{a, b = 1}^q J_{i, j}(a, b) \\ \\delta_{x_i^{(m)}, a} \\ \\delta_{x_j^{(m)}, b}\\right) = \\\\\n",
    "    & = \\frac{1}{Z^M} \\exp \\left(\\sum_{m = 1}^M \\sum_{i, j = 1}^N \\sum_{a, b = 1}^q J_{i, j}(a, b) \\ \\delta_{x_i^{(m)}, a}\\ \\delta_{x_j^{(m)}, b}\\right),\n",
    "\\end{align*}\n",
    "where\n",
    "\\begin{equation*}\n",
    "    Z(\\boldsymbol{J}) = \\sum_{x_1, \\dots, x_N = 1}^q \\exp \\left(\\sum_{i, j = 1}^N \\sum_{a, b = 1}^q J_{i, j}(a, b) \\ \\delta_{x_i, a} \\ \\delta_{x_j, b}\\right)\n",
    "\\end{equation*}\n",
    "is the normalization constant.\n",
    "To this aim we compute the log-likelihood (and divide by $M$), getting\n",
    "\\begin{equation*}\n",
    "    l\\left(\\boldsymbol{J}; \\left\\{\\boldsymbol{x}^{(m)}\\right\\}_{m = 1}^M\\right) \\coloneqq \\left(\\frac{1}{M} \\sum_{m=1}^M\\sum_{i, j = 1}^N\\sum_{a, b = 1}^q J_{i, j}(a, b) \\ \\delta_{x_i^{(m)}, a} \\ \\delta_{x_j^{(m)}, b}\\right) - \\log{Z(\\boldsymbol{J})}.\n",
    "\\end{equation*}\n",
    "Deriving w.r.t. $J_{i, j}(a, b)$, for fixed $i, j, a, b$, we get\n",
    "\\begin{equation*}\n",
    "    \\frac{\\partial l\\left(\\boldsymbol{J}; \\left\\{\\boldsymbol{x}^{(m)}\\right\\}_{m = 1}^M\\right)}{\\partial J_{i,j}(a,b)} \n",
    "\t= \\left(\\frac{1}{M} \\sum_{m = 1}^M \\delta_{x_i^{(m)}, a} \\ \\delta_{x_j^{(m)}, b}\\right) - \\frac{1}{Z(\\boldsymbol{J})} \\frac{\\partial Z(\\boldsymbol{J})}{\\partial J_{i, j}(a, b)},\n",
    "\\end{equation*}\n",
    "where\n",
    "\\begin{equation*}\n",
    "    \\frac{\\partial Z(\\boldsymbol{J})}{\\partial J_{i, j}(a, b)} = \\sum_{x_1, \\dots, x_N = 1}^q \\delta_{x_i,a} \\ \\delta_{x_j,b} \\ \\exp\\left(\\sum_{i, j = 1}^N\\sum_{a, b = 1}^q J_{i, j}(a, b) \\ \\delta_{x_i, a} \\ \\delta_{x_j, b}\\right).\n",
    "\\end{equation*}\n",
    "Plugging into the previous expression, we find that\n",
    "\\begin{align*}\n",
    "   \\frac{\\partial l\\left(\\boldsymbol{J}; \\left\\{\\boldsymbol{x}^{(m)}\\right\\}_{m = 1}^M\\right)}{\\partial J_{i,j}(a,b)} & = \\left(\\frac{1}{M} \\sum_{m = 1}^M \\delta_{x_i^{(m)}, a} \\ \\delta_{x_j^{(m)}, b}\\right) - \\sum_{x_1, \\dots, x_N = 1}^q \\delta_{x_i,a} \\ \\delta_{x_j,b} \\ \\frac{1}{Z(\\boldsymbol{J})} \\exp\\left(\\sum_{i, j = 1}^N\\sum_{a, b = 1}^q J_{i, j}(a, b) \\ \\delta_{x_i, a} \\ \\delta_{x_j, b}\\right) = \\\\\n",
    "   & = \\langle \\delta_{(x_i, x_j), (a, b)} \\rangle_{\\rm{data}} - \\langle \\delta_{(x_i, x_j), (a, b)} \\rangle_{\\rm{model}},\n",
    "\\end{align*}\n",
    "where $\\langle \\cdot \\rangle_{\\rm{data}}$ stands for the empirical mean of the observations, $\\langle \\cdot \\rangle_{\\rm{model}}$ is the mean of $\\delta_{(x_i, x_j), (a, b)}$ computed on the distribution of $\\boldsymbol{x} | \\boldsymbol{J}$ and $\\delta$ denotes again the Kronecker delta\n",
    "\\begin{equation*}\n",
    "\t\\delta_{(x_i, x_j), (a, b)} \\coloneqq \n",
    "\t\\begin{cases}\n",
    "\t\t1 & \\text{if } x_i = a \\text{ and } x_j = b \\\\\n",
    "\t\t0 & \\text{otherwise}\n",
    "\t\\end{cases}.\n",
    "\\end{equation*}\n",
    "Hence, in order to find the value of $J$ for which the function $\\mathcal{L}\\left(\\boldsymbol{J}; \\left\\{\\boldsymbol{x}^{(m)}\\right\\}_{m = 1}^M\\right)$ is maximised we have to impose\n",
    "\\begin{equation*}\n",
    "\t\\frac{\\partial l\\left(\\boldsymbol{J}; \\left\\{\\boldsymbol{x}^{(m)}\\right\\}_{m = 1}^M\\right)}{\\partial J_{i,j}(a,b)} = 0 \\iff \\langle \\delta_{(x_i, x_j), (a, b)} \\rangle_{\\rm{data}} = \\langle \\delta_{(x_i, x_j), (a, b)} \\rangle_{\\rm{model}}.\n",
    "\\end{equation*}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boltzmann machine learning scheme\n",
    "\n",
    "What we found can be exploited iteratively to estimate the coupling matrices through a gradient ascent algorithm (Boltzmann machine learning): $\\forall i, j, a, b$\n",
    "\\begin{align*}\n",
    "\t& J_{i, j}^{0}(a, b) = 0, \\\\\n",
    "\t& J_{i, j}^{t + 1}(a, b) \\leftarrow J_{i, j}^{t}(a, b) + \\lambda \\left[\\langle \\delta_{(x_i, x_j), (a, b)} \\rangle_{\\rm{data}} - \\langle \\delta_{(x_i, x_j), (a, b)} \\rangle_{\\rm{model}(t)}\\right], \\ \\forall t \\geq 0.\n",
    "\\end{align*}\n",
    "It is clear that at every step $t$ we should perform the computation of $\\langle \\delta_{(x_i, x_j), (a, b)} \\rangle_{\\rm{model(t)}}$ which costs $O(q^N)$, so we bypass the problem using a Metropolis-Hastings algorithm to sample from \n",
    "\\begin{equation*}\n",
    "\t\\pi_t\\left(\\boldsymbol{x}\\right) \\coloneqq \\frac{1}{Z(\\boldsymbol{J}^t)} \\exp\\left(\\sum_{i, j = 1}^N\\sum_{a, b = 1}^q J_{i, j}^{t}(x_i, x_j)\\right)\n",
    "\\end{equation*}\n",
    "and later estimate $\\langle \\delta_{(x_i, x_j), (a, b)} \\rangle_{\\rm{model(t)}}$:\n",
    "- `set` an initial condition $\\boldsymbol{x}^{0}$ (extract randomly from the $q^N$ possible configurations);\n",
    "- `for` $s \\in \\{1, \\dots, T_{\\rm{burn-in}} + T_{\\rm{tot}} \\times T_{\\rm{wait}}\\}$:\n",
    "\t1. `draw` $\\boldsymbol{x} \\sim p\\left(\\boldsymbol{x}|\\boldsymbol{x}^{(s - 1)}\\right)$ with \n",
    "\t\\begin{align*}\n",
    "\t\tp\\left(\\boldsymbol{x}|\\boldsymbol{x}^{(s - 1)}\\right) = \n",
    "\t\t\\begin{cases}\n",
    "\t\t\t\\frac{1}{qN} & \\text{if } \\boldsymbol{x} = \\left(x^{(s - 1)}_1, \\dots, x^{(s - 1)}_{i - 1}, \\left(x^{(s - 1)}_{i} + p\\right) \\text{ mod } q, x^{(s - 1)}_{i + 1}, \\dots, x^{(s - 1)}_{N}\\right), \\ \\forall i \\in {1, \\dots, N}, \\forall p \\in {1, \\dots, q}\\\\\n",
    "\t\t\t0 & \\text{otherwise}\n",
    "\t\t\\end{cases};\n",
    "\t\\end{align*}\n",
    "\t2. `compute` the acceptance ratio $a\\left(\\boldsymbol{x}|\\boldsymbol{x}^{(s - 1)}\\right)$:\n",
    "\t\\begin{align*}\n",
    "\t\ta\\left(\\boldsymbol{x}|\\boldsymbol{x}^{(s - 1)}\\right) & = \\min\\left[1, \\frac{p\\left(\\boldsymbol{x}^{(s - 1)}|\\boldsymbol{x}\\right) \\pi_t(x)}{p\\left(\\boldsymbol{x}|\\boldsymbol{x}^{(s - 1)}\\right) \\pi_t(x^{(s - 1)})}\\right] = \\\\\n",
    "\t\t& = \\min\\left[1, \\mathbf{1}_A(\\boldsymbol{x}) \\exp\\left(\\sum_{i, j = 1}^N J_{i, j}^{t}\\left(x_i, x_j\\right) - J_{i, j}^{t}\\left(x^{(s - 1)}_i, x^{(s - 1)}_j\\right)\\right)\\right],\n",
    "\t\\end{align*}\n",
    "\twhere we adopt the convention $\\frac{p\\left(\\boldsymbol{x}^{(s - 1)}|\\boldsymbol{x}\\right)}{p\\left(\\boldsymbol{x}|\\boldsymbol{x}^{(s - 1)}\\right)} = \\mathbf{1}_A(\\boldsymbol{x})$ with $A \\coloneqq \\left\\{x \\,|\\, p\\left(\\boldsymbol{x}|\\boldsymbol{x}^{(s - 1)}\\right) > 0\\right\\}$ (this notation has only a theoretical purpose).\n",
    "\tNow assuming \n",
    "\t\\begin{equation*}\n",
    "\t\t\\boldsymbol{x} = \\left(x^{(s - 1)}_1, \\dots, x^{(s - 1)}_{k - 1}, \\left(x^{(s - 1)}_{k} + p\\right) \\text{ mod } q, x^{(s - 1)}_{k + 1}, \\dots, x^{(s - 1)}_{N}\\right),\n",
    "\t\\end{equation*}\n",
    "\twe have\n",
    "\t\\begin{align*}\n",
    "\t\ta\\left(\\boldsymbol{x}|\\boldsymbol{x}^{(s - 1)}\\right) = \\min\\Bigg[1, & \\exp\\Bigg(\\sum_{i \\neq k} J_{i, k}^{t}\\left(x^{(s - 1)}_i, \\left(x^{(s - 1)}_{k} \\pm 1\\right) \\text{ mod } q\\right) - J_{i, k}^{t}\\left(x^{(s - 1)}_i, x^{(s - 1)}_k\\right) + \\\\\n",
    "\t\t& + \\sum_{j \\neq k} J_{k, j}^{t}\\left(\\left(x^{(s - 1)}_{k} + p\\right) \\text{ mod } q, x^{(s - 1)}_j\\right) - J_{k, j}^{t}\\left(x^{(s - 1)}_k, x^{(s - 1)}_j\\right) + \\\\\n",
    "\t\t& + \\left(J_{k, k}^{t}\\left(\\left(x^{(s - 1)}_{k} + p\\right) \\text{ mod } q, \\left(x^{(s - 1)}_{k} + p\\right) \\text{ mod } q\\right) - J_{k, k}^{t}\\left(x^{(s - 1)}_k, x^{(s - 1)}_k\\right)\\right) \\Bigg)\\Bigg].\n",
    "\t\\end{align*}\n",
    "\tBy simmetry of $\\boldsymbol{J}$, i.e. $J_{i, j}(a, b) = J_{j, i}(b, a)$, it holds\n",
    "\t\\begin{align*}\n",
    "\t\ta\\left(\\boldsymbol{x}|\\boldsymbol{x}^{(s - 1)}\\right) = \\min\\Bigg[1, & \\exp\\Bigg(2 \\sum_{i \\neq k} J_{i, k}^{t}\\left(x^{(s - 1)}_i, \\left(x^{(s - 1)}_{k} + p\\right) \\text{ mod } q\\right) - J_{i, k}^{t}\\left(x^{(s - 1)}_i, x^{(s - 1)}_k\\right) + \\\\\n",
    "\t\t& + \\left(J_{k, k}^{t}\\left(\\left(x^{(s - 1)}_{k} + p\\right) \\text{ mod } q, \\left(x^{(s - 1)}_{k} + p\\right) \\text{ mod } q\\right) - J_{k, k}^{t}\\left(x^{(s - 1)}_k, x^{(s - 1)}_k\\right)\\right) \\Bigg)\\Bigg].\n",
    "\t\\end{align*}\n",
    "\t3. `draw` $u \\sim U[0,1)$ (with the command `rand()`);\n",
    "\t4. `set`\n",
    "\t\\begin{equation*}\n",
    "\t\t\\boldsymbol{x}^{(s)} \\coloneqq \n",
    "\t\t\\begin{cases}\n",
    "\t\t\t\\boldsymbol{x} & \\text{if } u \\leq a \\\\\n",
    "\t\t\t\\boldsymbol{x}^{(s - 1)} & \\text{otherwise}\n",
    "\t\t\\end{cases};\n",
    "\t\\end{equation*}\n",
    "- estimate $\\langle \\delta_{(x_i, x_j), (a, b)} \\rangle_{\\rm{model(t)}}$ with $T_{\\rm{tot}}$ configurations obtained removing the burn-in and the waiting times:\n",
    "\\begin{equation*}\n",
    "\t\\langle \\delta_{(x_i, x_j), (a, b)} \\rangle_{\\rm{model(t)}} \\sim \\frac{1}{T_{\\rm{tot}}} \\sum_{s = 1}^{T_{\\rm{tot}}} \\delta_{(x^{(s)}_i, x^{(s)}_j), (a, b)}.\n",
    "\\end{equation*}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages\n",
    "using ProgressMeter\n",
    "using Distributions\n",
    "using DelimitedFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "function compute_stat(data::Matrix{Int64}, q::Int)\n",
    "\tcount = size(data, 1)\n",
    "\tn = size(data, 2)\n",
    "\tdelta_ijab = (zeros(q, q), ) .* ones(n, n)\n",
    "\n",
    "\tfor i in 1:n, j in 1:n, a in 1:q, b in 1:q\n",
    "\t\tfor s in 1:count\n",
    "\t\t\tdelta_ijab[i, j][a, b] += (data[s, i] == a) * (data[s, j] == b)\n",
    "\t\tend\n",
    "\tend\n",
    "\tdelta_ijab /= count\n",
    "\n",
    "\treturn delta_ijab\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "function maxabs_matmat(m::Matrix{Matrix{Float64}})\n",
    "\tmax = -1\n",
    "\tfor r in 1:size(m, 1), c in 1:size(m, 2)\n",
    "\t\tif maximum(abs.(m[r, c])) > max\n",
    "\t\t\tmax = maximum(abs.(m[r, c]))\n",
    "\t\tend\n",
    "\tend\n",
    "\n",
    "\treturn(max)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "function metropolis_hastings_step(x::Vector{Int64}, J::Matrix{Matrix{Float64}})\n",
    "\tn = length(x)\n",
    "\n",
    "\t# 1.\n",
    "\tk = rand(1:n)\n",
    "\tp = rand(1:q)\n",
    "\t\n",
    "\t# 2.\n",
    "\ta = 0\n",
    "\txk_new = mod((x[k] + p) - 1, q) + 1\n",
    "\tfor i in 1:n\n",
    "\t\tif i != k\n",
    "\t\t\ta += J[i, k][x[i], xk_new] - J[i, k][x[i], x[k]]\n",
    "\t\tend\n",
    "\tend\n",
    "\ta *= 2\n",
    "\ta += J[k, k][xk_new, xk_new] - J[k, k][x[k], x[k]]\n",
    "\ta = exp(a)\n",
    "\ta = min(1, a)\n",
    "\n",
    "\t# 3/4.\n",
    "\tif rand() < a\n",
    "\t\t# mod with indexes starting from 1\n",
    "\t\tx[k] = xk_new\n",
    "\tend\n",
    "\t\n",
    "\treturn x\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "function boltzmann_ml(delta_ijab_data::Matrix{Matrix{Float64}}, J::Matrix{Matrix{Float64}}, \n",
    "\t\t\t\t\tt_burnin::Int64, t_tot::Int64, t_wait::Int64, t_max::Int64;\n",
    "\t\t\t\t\tλ::Float64 = 0.1, ε_max::Float64 = 1e-2)\n",
    "\tn = size(delta_ijab_data, 1)\n",
    "\tq = size(delta_ijab_data[1, 1], 1)\n",
    "\t\n",
    "\tx = sample(collect(1:q), n, replace = true)\n",
    "\tx_model = zeros(Int64, t_tot, n)\n",
    "\tdelta_ijab_model = (zeros(q, q), ) .* ones(n, n)\n",
    "\n",
    "\tt = 0\n",
    "\tε = 1\n",
    "\tProgressMeter.ijulia_behavior(:clear)\n",
    "\tp = ProgressUnknown(\"learning...\", spinner = true)\n",
    "\n",
    "\twhile t <= t_max && ε > ε_max\n",
    "\t\tt += 1\n",
    "\t\tfill!(x_model, 0)\n",
    "\t\tx = sample(collect(1:q), n, replace = true)\n",
    "\n",
    "\t\tfor s in 1:t_burnin\n",
    "\t\t\tx = metropolis_hastings_step(x, J)\n",
    "\t\tend\n",
    "\t\tfor s in 1:t_tot\n",
    "\t\t\tfor r in 1:t_wait\n",
    "\t\t\t\tx = metropolis_hastings_step(x, J)\n",
    "\t\t\tend\n",
    "\t\t\tx_model[s, :] = x\n",
    "\t\tend\n",
    "\t\t\n",
    "\t\tdelta_ijab_model = compute_stat(x_model, q)\n",
    "\t\tJ = J + λ .* (delta_ijab_data - delta_ijab_model)\n",
    "\t\n",
    "\t\tε = maxabs_matmat(delta_ijab_data .- delta_ijab_model)\n",
    "\n",
    "\t\tif mod(t, t_max ÷ 10) == 0\n",
    "\t\t\tProgressMeter.next!(p; showvalues = [(:t, t), (:ε, ε)], spinner = \"⠋⠙⠹⠸⠼⠴⠦⠧⠇⠏\")\n",
    "\t\tend\n",
    "\tend\n",
    "\n",
    "\tProgressMeter.finish!(p)\n",
    "\n",
    "\treturn delta_ijab_model, J, x_model, t, ε\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "n = 5\n",
    "q = 4\n",
    "\n",
    "# the estimation is also acceptable by imposing\n",
    "# t_burnin = 250\n",
    "# t_tot = 250\n",
    "# t_wait = 100\n",
    "# t_max = 500\n",
    "\n",
    "t_burnin = 500\n",
    "t_tot = 500\n",
    "t_wait = 100\n",
    "t_max = 500;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delta_ijab_data\n",
    "x_data = readdlm(\"data.dat\", Int)\n",
    "delta_ijab_data = compute_stat(x_data, q);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[32m⠋ learning... \t Time: 0:00:06\u001b[39m\u001b[K\r\n",
      "\u001b[34m  t:  100\u001b[39m\u001b[K\r\n",
      "\u001b[34m  ε:  0.045499999999999985\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[K\u001b[A\r\u001b[K\u001b[A\r\u001b[32m⠙ learning... \t Time: 0:00:09\u001b[39m\u001b[K\r\n",
      "\u001b[34m  t:  150\u001b[39m\u001b[K\r\n",
      "\u001b[34m  ε:  0.03\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[K\u001b[A\r\u001b[K\u001b[A\r\u001b[32m⠹ learning... \t Time: 0:00:12\u001b[39m\u001b[K\r\n",
      "\u001b[34m  t:  200\u001b[39m\u001b[K\r\n",
      "\u001b[34m  ε:  0.0325\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[K\u001b[A\r\u001b[K\u001b[A\r\u001b[32m⠸ learning... \t Time: 0:00:15\u001b[39m\u001b[K\r\n",
      "\u001b[34m  t:  250\u001b[39m\u001b[K\r\n",
      "\u001b[34m  ε:  0.03149999999999999\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[K\u001b[A\r\u001b[K\u001b[A\r\u001b[32m⠼ learning... \t Time: 0:00:18\u001b[39m\u001b[K\r\n",
      "\u001b[34m  t:  300\u001b[39m\u001b[K\r\n",
      "\u001b[34m  ε:  0.038000000000000006\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[K\u001b[A\r\u001b[K\u001b[A\r\u001b[32m⠴ learning... \t Time: 0:00:21\u001b[39m\u001b[K\r\n",
      "\u001b[34m  t:  350\u001b[39m\u001b[K\r\n",
      "\u001b[34m  ε:  0.04500000000000001\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[K\u001b[A\r\u001b[K\u001b[A\r\u001b[32m⠦ learning... \t Time: 0:00:24\u001b[39m\u001b[K\r\n",
      "\u001b[34m  t:  400\u001b[39m\u001b[K\r\n",
      "\u001b[34m  ε:  0.03150000000000003\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[K\u001b[A\r\u001b[K\u001b[A\r\u001b[32m⠧ learning... \t Time: 0:00:26\u001b[39m\u001b[K\r\n",
      "\u001b[34m  t:  450\u001b[39m\u001b[K\r\n",
      "\u001b[34m  ε:  0.057999999999999996\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[K\u001b[A\r\u001b[K\u001b[A\r\u001b[32m⠇ learning... \t Time: 0:00:29\u001b[39m\u001b[K\r\n",
      "\u001b[34m  t:  500\u001b[39m\u001b[K\r\n",
      "\u001b[34m  ε:  0.04099999999999998\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[K\u001b[A\r\u001b[K\u001b[A\r\u001b[32m✓ learning... \t Time: 0:00:29\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# boltzmann ml scheme\n",
    "J = (zeros(q, q), ) .* ones(n, n)\n",
    "delta_ijab_model, J, x_model, t, ε = boltzmann_ml(delta_ijab_data, J, t_burnin, t_tot, t_wait, t_max);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Computation of $\\mathcal{F}_{i, j}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frobenius norm\n",
    "\n",
    "In the previous point we estimated the parameter $\\boldsymbol{J}$ through a Boltzmann machine learning scheme and now we call that parameter $\\boldsymbol{J}^\\star$.\n",
    "\n",
    "We define for $i, j \\in \\{1, \\dots, N\\}$ the Frobenius norm of $J_{i, j}^{\\star}$ as\n",
    "\\begin{equation*}\n",
    "\t\\mathcal{F}_{i, j} = \\sqrt{\\sum_{a, b = 1}^{q} J^{\\star}_{i, j}(a, b)^2}.\n",
    "\\end{equation*}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages\n",
    "using Printf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "function frobenius_norm(m::Matrix{Float64})\n",
    "\tf = 0\n",
    "\tfor r in 1:size(m, 1), c in 1:size(m, 2)\n",
    "\t\tf += m[r, c]^2\n",
    "\tend\n",
    "\tf = sqrt(f)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×5 Matrix{Float64}:\n",
       " 0.0180724  0.729118   0.848124   0.165538   0.118669\n",
       " 0.729118   0.0212211  0.132106   0.809545   0.106892\n",
       " 0.848124   0.132106   0.0159118  0.110478   0.874686\n",
       " 0.165538   0.809545   0.110478   0.0432372  0.905757\n",
       " 0.118669   0.106892   0.874686   0.905757   0.0154436"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "5×2 Matrix{Int64}:\n",
       " 2  3\n",
       " 1  4\n",
       " 1  5\n",
       " 2  5\n",
       " 3  4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# computation of f_{i, j} for all i, j\n",
    "f = zeros(n, n)\n",
    "for i in 1:n, j in 1:n\n",
    "\tf[i, j] = frobenius_norm(J[i, j])\n",
    "end\n",
    "display(f)\n",
    "\n",
    "neighbors = [Int64[], Int64[], Int64[], Int64[], Int64[]]\n",
    "for i in 1:n, j in 1:n\n",
    "\tif (f[i, j] > 0.2)\n",
    "\t\tappend!(neighbors[i], [j])\n",
    "\tend\n",
    "end\n",
    "neighbors = mapreduce(permutedims, vcat, neighbors)\n",
    "display(neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×2 Matrix{Int64}:\n",
       " 2  3\n",
       " 1  4\n",
       " 1  5\n",
       " 2  5\n",
       " 3  4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "groundtruth = readdlm(\"groundtruth.dat\", Int)\n",
    "display(groundtruth)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
