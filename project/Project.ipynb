{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum likelihood estimator\n",
    "\n",
    "Our goal is to find the set of parameters $J_{i, j}(a, b)$ for all $i, j \\in \\{1, \\dots, N\\}$, $a, b \\in \\{1, \\dots, q\\}$ that maximises the likelihood\n",
    "\\begin{align*}\n",
    "    \\mathcal{L}\\left(\\boldsymbol{J}; \\left\\{\\boldsymbol{x}^{(m)}\\right\\}_{m = 1}^M\\right) & \\coloneqq \\mathbb{P}\\left(\\left\\{\\boldsymbol{x}^{(m)}\\right\\}_{m = 1}^M; \\boldsymbol{J}\\right)\n",
    "    = \\prod_{m = 1}^{M} \\mathbb{P} \\left(\\boldsymbol{x}^{(m)}; \\boldsymbol{J}\\right) = \\\\\n",
    "    & = \\prod_{m = 1}^{M} \\frac{1}{Z(\\boldsymbol{J})} \\exp\\left(\\sum_{i, j = 1}^N\\sum_{a, b = 1}^q J_{i, j}(a, b) \\ \\delta_{x_i^{(m)}, a} \\ \\delta_{x_j^{(m)}, b}\\right) = \\\\\n",
    "    & = \\frac{1}{Z^M} \\exp \\left(\\sum_{m = 1}^M \\sum_{i, j = 1}^N \\sum_{a, b = 1}^q J_{i, j}(a, b) \\ \\delta_{x_i^{(m)}, a}\\ \\delta_{x_j^{(m)}, b}\\right),\n",
    "\\end{align*}\n",
    "where\n",
    "\\begin{equation*}\n",
    "    Z(\\boldsymbol{J}) = \\sum_{x_1, \\dots, x_N = 1}^q \\exp \\left(\\sum_{i, j = 1}^N \\sum_{a, b = 1}^q J_{i, j}(a, b) \\ \\delta_{x_i, a} \\ \\delta_{x_j, b}\\right)\n",
    "\\end{equation*}\n",
    "is the normalization constant.\n",
    "To this aim we compute the log-likelihood (and divide by $M$), getting\n",
    "\\begin{equation*}\n",
    "    l\\left(\\boldsymbol{J}; \\left\\{\\boldsymbol{x}^{(m)}\\right\\}_{m = 1}^M\\right) \\coloneqq \\left(\\frac{1}{M} \\sum_{m=1}^M\\sum_{i, j = 1}^N\\sum_{a, b = 1}^q J_{i, j}(a, b) \\ \\delta_{x_i^{(m)}, a} \\ \\delta_{x_j^{(m)}, b}\\right) - \\log{Z(\\boldsymbol{J})}.\n",
    "\\end{equation*}\n",
    "Deriving w.r.t. $J_{i, j}(a, b)$, for fixed $i, j, a, b$, we get\n",
    "\\begin{equation*}\n",
    "    \\frac{\\partial l\\left(\\boldsymbol{J}; \\left\\{\\boldsymbol{x}^{(m)}\\right\\}_{m = 1}^M\\right)}{\\partial J_{i,j}(a,b)} \n",
    "\t= \\left(\\frac{1}{M} \\sum_{m = 1}^M \\delta_{x_i^{(m)}, a} \\ \\delta_{x_j^{(m)}, b}\\right) - \\frac{1}{Z(\\boldsymbol{J})} \\frac{\\partial Z(\\boldsymbol{J})}{\\partial J_{i, j}(a, b)},\n",
    "\\end{equation*}\n",
    "where\n",
    "\\begin{equation*}\n",
    "    \\frac{\\partial Z(\\boldsymbol{J})}{\\partial J_{i, j}(a, b)} = \\sum_{x_1, \\dots, x_N = 1}^q \\delta_{x_i,a} \\ \\delta_{x_j,b} \\ \\exp\\left(\\sum_{i, j = 1}^N\\sum_{a, b = 1}^q J_{i, j}(a, b) \\ \\delta_{x_i, a} \\ \\delta_{x_j, b}\\right)\n",
    "\\end{equation*}.\n",
    "Plugging into the previous expression, we find that\n",
    "\\begin{align*}\n",
    "   \\frac{\\partial l\\left(\\boldsymbol{J}; \\left\\{\\boldsymbol{x}^{(m)}\\right\\}_{m = 1}^M\\right)}{\\partial J_{i,j}(a,b)} & = \\left(\\frac{1}{M} \\sum_{m = 1}^M \\delta_{x_i^{(m)}, a} \\ \\delta_{x_j^{(m)}, b}\\right) - \\sum_{x_1, \\dots, x_N = 1}^q \\delta_{x_i,a} \\ \\delta_{x_j,b} \\ \\frac{1}{Z(\\boldsymbol{J})} \\exp\\left(\\sum_{i, j = 1}^N\\sum_{a, b = 1}^q J_{i, j}(a, b) \\ \\delta_{x_i, a} \\ \\delta_{x_j, b}\\right) = \\\\\n",
    "   & = \\langle \\delta_{(x_i, x_j), (a, b)} \\rangle_{\\rm{data}} - \\langle \\delta_{(x_i, x_j), (a, b)} \\rangle_{\\rm{model}},\n",
    "\\end{align*}\n",
    "where $\\langle \\cdot \\rangle_{\\rm{data}}$ stands for the empirical mean of the observations, $\\langle \\cdot \\rangle_{\\rm{model}}$ is the mean of $\\delta_{(x_i, x_j), (a, b)}$ computed on the distribution of $\\boldsymbol{x} | \\boldsymbol{J}$ and $\\delta$ denotes again the Kronecker delta\n",
    "\\begin{equation*}\n",
    "\t\\delta_{(x_i, x_j), (a, b)} \\coloneqq \n",
    "\t\\begin{cases}\n",
    "\t\t1 & \\text{if } x_i = a \\text{ and } x_j = b \\\\\n",
    "\t\t0 & \\text{otherwise}\n",
    "\t\\end{cases}.\n",
    "\\end{equation*}\n",
    "Hence, in order to find the value of $J$ for which the function $\\mathcal{L}\\left(\\boldsymbol{J}; \\left\\{\\boldsymbol{x}^{(m)}\\right\\}_{m = 1}^M\\right)$ is maximised we have to impose\n",
    "\\begin{equation*}\n",
    "\t\\frac{\\partial l\\left(\\boldsymbol{J}; \\left\\{\\boldsymbol{x}^{(m)}\\right\\}_{m = 1}^M\\right)}{\\partial J_{i,j}(a,b)} = 0 \\iff \\langle \\delta_{(x_i, x_j), (a, b)} \\rangle_{\\rm{data}} = \\langle \\delta_{(x_i, x_j), (a, b)} \\rangle_{\\rm{model}}.\n",
    "\\end{equation*}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boltzmann machine learning scheme\n",
    "\n",
    "What we found can be exploited iteratively to estimate the coupling matrices through a gradient ascent algorithm (Boltzmann machine learning):\n",
    "\\begin{align*}\n",
    "\t& J_{i, j}^{0}(a, b) = 0, \\\\\n",
    "\t& J_{i, j}^{t + 1}(a, b) \\leftarrow J_{i, j}^{t}(a, b) + \\lambda \\left[\\langle \\delta_{(x_i, x_j), (a, b)} \\rangle_{\\rm{data}} - \\langle \\delta_{(x_i, x_j), (a, b)} \\rangle_{\\rm{model}(t)}\\right], \\ \\forall t \\geq 0.\n",
    "\\end{align*}\n",
    "<!-- question: nell'inverse ising problem usa lo stesso learning parameter per tutti i parametri che vuole inferire, anche qui conviene fare così? perchè in teoria si potrebbe settare un learning parameter diverso per ogni J_{i, j}(a, b)... -->\n",
    "It is clear that at every step $t$ we should perform the computation of $\\langle \\delta_{(x_i, x_j), (a, b)} \\rangle_{\\rm{model(t)}}$ which costs $O(q^N)$, so we bypass the problem using a Metropolis-Hastings algorithm to sample from \n",
    "\\begin{equation*}\n",
    "\t\\pi_t\\left(\\boldsymbol{x}\\right) \\coloneqq \\frac{1}{Z(\\boldsymbol{J}^t)} \\exp\\left(\\sum_{i, j = 1}^N\\sum_{a, b = 1}^q J_{i, j}^{t}(x_i, x_j)\\right)\n",
    "\\end{equation*}\n",
    "and later estimate $\\langle \\delta_{(x_i, x_j), (a, b)} \\rangle_{\\rm{model(t)}}$:\n",
    "- `set` an initial condition $\\boldsymbol{x}^{0}$ (extract randomly from the $q^N$ possible configurations);\n",
    "- `for` $s \\in \\{1, \\dots, T_{\\rm{burn-in}} + T_{\\rm{tot}} \\times T_{\\rm{wait}}\\}$:\n",
    "\t1. `draw` $\\boldsymbol{x} \\sim p\\left(\\boldsymbol{x}|\\boldsymbol{x}^{(s - 1)}\\right)$ with \n",
    "\t\\begin{align*}\n",
    "\t\tp\\left(\\boldsymbol{x}|\\boldsymbol{x}^{(s - 1)}\\right) = \n",
    "\t\t\\begin{cases}\n",
    "\t\t\t\\frac{1}{2N} & \\text{if } \\boldsymbol{x} = \\left(x^{(s - 1)}_1, \\dots, x^{(s - 1)}_{i - 1}, \\left(x^{(s - 1)}_{i} \\pm 1\\right) \\text{ mod } q, x^{(s - 1)}_{i + 1}, \\dots, x^{(s - 1)}_{N}\\right), \\ \\forall i \\in {1, \\dots, N} \\\\\n",
    "\t\t\t0 & \\text{otherwise}\n",
    "\t\t\\end{cases}\n",
    "\t\\end{align*};\n",
    "\t<!-- remark: qui ho usato come neighborhood la cosa più semplice, magari si puù fare di meglio -->\n",
    "\t2. `compute` the acceptance ratio $a\\left(\\boldsymbol{x}|\\boldsymbol{x}^{(s - 1)}\\right)$:\n",
    "\t\\begin{align*}\n",
    "\t\ta\\left(\\boldsymbol{x}|\\boldsymbol{x}^{(s - 1)}\\right) & = \\min\\left[1, \\frac{p\\left(\\boldsymbol{x}^{(s - 1)}|\\boldsymbol{x}\\right) \\pi_t(x)}{p\\left(\\boldsymbol{x}|\\boldsymbol{x}^{(s - 1)}\\right) \\pi_t(x^{(s - 1)})}\\right] = \\\\\n",
    "\t\t& = \\min\\left[1, \\mathbf{1}_A(\\boldsymbol{x}) \\exp\\left(\\sum_{i, j = 1}^N J_{i, j}^{t}\\left(x_i, x_j\\right) - J_{i, j}^{t}\\left(x^{(s - 1)}_i, x^{(s - 1)}_j\\right)\\right)\\right],\n",
    "\t\\end{align*}\n",
    "\twhere we adopt the convention $\\frac{p\\left(\\boldsymbol{x}^{(s - 1)}|\\boldsymbol{x}\\right)}{p\\left(\\boldsymbol{x}|\\boldsymbol{x}^{(s - 1)}\\right)} = \\mathbf{1}_A(\\boldsymbol{x})$ with $A \\coloneqq \\left\\{x \\,|\\, p\\left(\\boldsymbol{x}|\\boldsymbol{x}^{(s - 1)}\\right) > 0\\right\\}$ (this notation has only a theoretical purpose).\n",
    "\tNow assuming \n",
    "\t\\begin{equation*}\n",
    "\t\t\\boldsymbol{x} = \\left(x^{(s - 1)}_1, \\dots, x^{(s - 1)}_{k - 1}, \\left(x^{(s - 1)}_{k} \\pm 1\\right) \\text{ mod } q, x^{(s - 1)}_{k + 1}, \\dots, x^{(s - 1)}_{N}\\right),\n",
    "\t\\end{equation*}\n",
    "\twe have\n",
    "\t\\begin{align*}\n",
    "\t\ta\\left(\\boldsymbol{x}|\\boldsymbol{x}^{(s - 1)}\\right) = \\min\\Bigg[1, & \\exp\\Bigg(\\sum_{i \\neq k} J_{i, k}^{t}\\left(x^{(s - 1)}_i, \\left(x^{(s - 1)}_{k} \\pm 1\\right) \\text{ mod } q\\right) - J_{i, k}^{t}\\left(x^{(s - 1)}_i, x^{(s - 1)}_k\\right) + \\\\\n",
    "\t\t& + \\sum_{j \\neq k} J_{k, j}^{t}\\left(\\left(x^{(s - 1)}_{k} \\pm 1\\right) \\text{ mod } q, x^{(s - 1)}_j\\right) - J_{i, k}^{t}\\left(x^{(s - 1)}_k, x^{(s - 1)}_j\\right) + \\\\\n",
    "\t\t& + \\left(J_{k, k}^{t}\\left(\\left(x^{(s - 1)}_{k} \\pm 1\\right) \\text{ mod } q, \\left(x^{(s - 1)}_{k} \\pm 1\\right) \\text{ mod } q\\right) - J_{k, k}^{t}\\left(x^{(s - 1)}_k, x^{(s - 1)}_k\\right)\\right) \\Bigg)\\Bigg].\n",
    "\t\\end{align*}\n",
    "\t<!-- question: non mi pare di aver fatto errori di calcolo e non mi pare che questa bestia si semplifichi, quindi credo che ci siano delle simmetrie da assumere tipo J_{i, j} = J_{j, i} e anche J_{i, j} a loro volta simmetriche, però onestamente non saprei cosa assumere -->\n",
    "\t3. `draw` $u \\sim U[0,1)$ (with the command `rand()`);\n",
    "\t4. `set`\n",
    "\t\\begin{equation*}\n",
    "\t\t\\boldsymbol{x}^{(s)} \\coloneqq \n",
    "\t\t\\begin{cases}\n",
    "\t\t\t\\boldsymbol{x} & \\text{if } u \\leq a \\\\\n",
    "\t\t\t\\boldsymbol{x}^{(s - 1)} & \\text{otherwise}\n",
    "\t\t\\end{cases};\n",
    "\t\\end{equation*}\n",
    "- estimate $\\langle \\delta_{(x_i, x_j), (a, b)} \\rangle_{\\rm{model(t)}}$ with $T_{\\rm{tot}}$ configurations obtained removing the burn-in and the waiting times:\n",
    "\\begin{equation*}\n",
    "\t\\langle \\delta_{(x_i, x_j), (a, b)} \\rangle_{\\rm{model(t)}} \\sim \\sum_{s = 1}^{T_{\\rm{tot}}} \\delta_{(x^{(s)}_i, x^{(s)}_j), (a, b)}.\n",
    "\\end{equation*}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
